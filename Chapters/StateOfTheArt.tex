\chapter{State of the art} % Main chapter title
\label{StateOfTheArt}

\section{Synthèse}
Dans ce chapitre, nous présentons le travail existant dans les algorithmes de rendu de volume, les stratégies de gestion de l'occlusion et le rendu de volume dans les appareils mobiles (réalité mixte, réalité virtuelle et réalité augmentée). Premièrement, nous présentons les algorithmes existants pour rendre des jeux de données volumétriques pouvant être classés en deux types principaux: le rendu de volume direct (DVR) et les algorithmes d'ajustement de surface. Ensuite, nous discutons des stratégies existantes de gestion de l'occlusion. En effet, nous parlons de la fonction de transfert, de la segmentation, des déformations ainsi que des techniques de Focus+Context. Enfin, nous décrivons les travaux précédents sur le rendu de volume dans les appareils mobiles. Plus précisément, nous traitons de la réalité mixte, de la réalité virtuelle et de la réalité augmentée.

Les algorithmes fondamentaux de visualisation de volume décrits
ci-dessous se divisent en deux catégories principales:

\begin{itemize}

\item Algorithmes de rendu de volume direct (DVR): ils incluent des approches telles que le lancer de rayons, le mappage de texture, le splatting et le rendu du V-buffer. Ces méthodes sont caractérisées par la correspondance des éléments directement dans l'espace écran sans utiliser de primitives géométriques comme représentation intermédiaire (\cite{Stegmaier:2005:SFV:2386472.2386498}).

\item Algorithmes d'ajustement de surface (SF): ils s'appellent aussi isosurfacing ou feature-extraction. Ces algorithmes consistent à adapter des primitives de surface telles que
Polygones ou patches sur des surfaces de contour à valeur constante dans des jeux de données volumétriques. Les primitives de surface utilisées sont la plupart du temps planes (\cite{Lorensen:1987:MCA}).

\end{itemize}

Malgré tous les travaux antérieurs, en l'occurrence \cite{1250400}, \cite{Sonnet:2004:IEA:989863.989871} ,  \cite{1532818} ,  \cite{4015467}, \cite{Correa:2006:FAV:1187627.1187827},  \cite{Correa:2007:IDD:1313046.1313163}, \cite{5613463}, \cite{Hsu:2011:RFM:2070781.2024165}, \cite{6787171}, \cite{7120994} , il est toujours difficile de sélectionner efficacement une cible et de retirer tous les objets intermédiaires qui crée de l'occlusion. Plus précisément, la plupart des techniques de gestion d'occlusion existantes ne répondent pas simultanément à toutes les exigences suivantes: Créer rapidement une vue non obstruée de la cible (R1), permettre une exploration locale flexible de la zone cible (R2), conserver le contexte dans lequel la cible est visuellement contenue (R3), et gèrer les jeux de données où la cible et les objets gênants ne peuvent pas
être séparé par des manipulations de la fonction de transfert (R4).



De nombreuses approches de gestion de l'occlusion ont été proposées \, \cite{4483791}. Plusieurs fenêtres peuvent afficher les données avec différentes perspectives \, \cite{WangBaldonado:2000:GUM:345513.345271}. Mais cela n'aide pas lorsque la cible est fortement sujette à l'occlusion aux niveaux de \emph{tous} les points de vue possibles (R4). Les méthodes de rayon X virtuelles rendent les cibles visibles en transformant les semi-transparents  \, \cite{Burns:2008:ACC:1457515.1409107}. \cite{4015450} se concentre de manière interactive sur des zones spécifiques des données qu'il libère de l'occlusion en modulant la transparence. Correa et Ma \, \cite{5416704} ont proposé des fonctions de transfert basées sur la visibilité pour optimiser la visibilité des intervalles de données sélectionnés. La conception de bonnes fonctions de transfert est toujours un défi (R2) en général: par exemple, lors de l'inspection des bagages, une stratégie de dissimulation consiste à masquer une menace parmi les objets de même densité, \cite{7819413}. Sortir une tumeur de l'occlusion du tissu environnant de densité similaire dans les examens médicaux est une situation similaire \, \cite{CGF:CGF12927}. \cite{CGF:CGF979} ont également considéré l'occurrence des voxels sur le lancer de rayon, en plus de leurs densités et positions. \cite{6787171} a supprimé les objets gênants dans les images 2D, les volumes (les repoussant) dans une zone de mise au point. Les occlusions sont sélectionnées en fonction des plages de valeurs de données, nous avons donc ici la même limitation que ClearView (R4). \cite{Li:2012:LVV:2425296.2425325} a proposé le déballage virtuel des bagages où les cibles sont dégagées en éloignant interactivement les objets gênants. Ceci, cependant, modifie le \emph{contexte} (position relative, connectivité) où l'occlusion se trouve (R3). Récemment, une exploration interactive des données volumétriques par manipulation directe de voxels a été proposée, \cite{7819413}. L'extension de telles approches dans un environnement de DVR à des déformations plus complexes ou à des modifications de la zone d'intérêt est un challenge en terme de rapidité de calculs (R1).

Les lentilles pour le DVR sont confrontées à des problèmes de sélection spatiale et d’occlusion. \cite{1532818} les corrige en rendant les occlusions avec une opacité plus faible et en agrandissant les objets d'intérêt pré-calculées de manière interactive ou automatique dans un jeu de données pré-segmenté. Ceci, cependant, ne fournit pas un moyen (interactif) de traiter les cibles et les objets gênants de densité similaire (R4) et ne propose pas l'exploration locale du contexte cible (R2). GlyphLens \, \cite{7539643} supprime les glyphes en les faisant passer par une animation. Cependant, cela ne couvre que les visualisations basées sur les glyphes et non sur les methodes de DVR.

Les lentilles peuvent créer des discontinuités entre leurs zones intérieures et extérieures. Les déformations peuvent être une solution à cela.
 \cite{Hsu:2011:RFM:2070781.2024165} crée des déformations flexibles par rayons non linéairement échantillonnés pour projeter des objets en douceur à plusieurs niveaux de détail. Cependant, ceci est coûteux en calcul et loin du temps réel (R1). Les vues éclatées sont utilisées pour partitionner un volume en plusieurs segments \, \cite{Sonnet:2004:IEA:989863.989871,4015467}. Cela réduit fortement l'occlusion mais déforme le contexte (R3) et nécessite une sorte de pré-segmentation des données qui prend du temps pour être calculée (R1). \cite{Correa:2007:IDD:1313046.1313163, Correa:2006:FAV:1187627.1187827} permet de manipuler la géométrie d'un objet de données. \ cite {1250400} effectuer des déformations en utilisant le pelage pour voir les parties cachées des données. Ces techniques suppriment les informations contextuelles potentiellement importantes entourant la cible, ce qui rend difficile de voir comment la cible est intégrée dans son contexte (R2).


\begin{table}
\centering
\small
\begin{tabular}{ |c|c|c|c|c|c| }
\hline
\textbf{Technique} & \textbf{R1} & \textbf{R2} & \textbf{R3} & \textbf{R4} & \textbf{Cas d'utilisation}\\
\hline
\hline
\cite{1250400} & + & -- & + & + & 1\\
\cite{Sonnet:2004:IEA:989863.989871} & +/- & -- & + & +/- & 5\\
\cite{1532818} & + & -- & + & +/- & 6\\
\cite{4015467} & +/- & -- & -- & + & 6\\
\cite{Correa:2006:FAV:1187627.1187827} & + & -- & + & + & 8\\
\cite{Correa:2007:IDD:1313046.1313163} & + & -- & + & + & 8\\
\cite{5613463} & + & -- & + & -- & 5\\
\cite{Hsu:2011:RFM:2070781.2024165} & -- & +/- & + & + & 4\\
\cite{6787171} & + & +/- & + & -- & 6\\
\cite{7120994} & + & -- & + & -- & 5\\
\hline
\end{tabular}
\caption{Etat de l'art \emph{vs} exigences R1...R4 ('+': Bon, '+/-': moyen, '--': faible) et le nombre de cas d'utilisation pour illustrer ces méthodes.}
\label{tab:methodsfr}
\end{table}

Les déformations peuvent révéler des caractéristiques de données spécifiques en utilisant une segmentation précalculée. \cite{7332955} a proposé une lentille de déformation qui déplace les lignes de courant pour observer la partie interne des faisceaux de courant. D'autres techniques ont effectué des déformations en utilisant des métaphores chirurgicales, telles que \cite{4069230, Correa:2006:FAV:1187627.1187827} pour montrer les parties cachées d'un volume. De telles techniques n'offrent pas d'outils de manipulation locale du point de vue qui permettent de voir une cible sous plusieurs perspectives (R2) tout en gardant le contexte global.

L'utilisation de rayons non linéaires est un autre moyen d'éviter les objets gênants. \cite{5613463} propose des rayons courbés (Bézier) pour supporter ceci. \cite{7120994} affine cela pour créer des vues multi-perspectives. Ces approches requièrent une planification minutieuse du tracé des rayons, de sorte qu'elles ne visent pas directement un effet de lentille (R2). De plus, la cible doit être accessible depuis le point de vue via un chemin (éventuellement courbe) (R4). En outre, à partir des exemples présentés, seulement deux exemples de modèles de DVR sont donnés; il semblerait que ces techniques concernent principalement la gestion de l'occlusion dans les grandes scènes 3D polygonales telles que les modèles de terrain et de ville.

\autoref{tab:methodsfr} résume les principaux avantages et limites d'un ensemble de techniques d'exploration volumétriques sélectionnées parmi celles mentionnées ci-dessus qui sont proches de notre proposition. Comme on peut le voir, aucune des techniques ne remplit simultannément  toutes les exigences. Le tableau répertorie également le nombre de jeux de données et / ou de cas d'utilisation différents sur lesquels ces techniques ont été testées. Nous validerons notre proposition sur un nombre similaire de cas d'utilisation dans les chapitres suivants.

\NewPage

\section{Introduction}

In this Chapter, we carry out a presentation of existing work in volume rendering algorithms, occlusion management strategies, and volume rendering in mobile devices( Mixed Reality, Virtual Reality, and Augmented Reality). First, we present the existing algorithms to render volumetric data-sets that can be categorized into two major types: Direct Volume Rendering (DVR), and Surface-fitting algorithms. Then we discuss the existing strategies of occlusion management. Actually, we talk about the transfer function, the segmentation, deformations as well as Focus+context techniques. Finally, we describe the previous work on volume rendering in mobile devices. More precisely, we deal with Mixed Reality, Virtual Reality, and Augmented Reality.



\section{Volume rendering algorithms}

The fundamental volume visualization algorithms described
below fall into two main categories: 

\begin{itemize}

\item Direct volume rendering (DVR) algorithms: they include approaches such as ray-casting, texture mapping, splatting, and V-buffer rendering. Splatting and V-buffer are also called projection methods. These methods are characterized by mapping elements directly into screen space without using geometric primitives as an intermediate representation.

\item Surface-fitting (SF) algorithms: they are also called isosurfacing or feature-extraction. These algorithms consist in fitting surface primitives such as
polygons or patches to constant-value contour surfaces in volumetric datasets. The surface primitives used are most of the time planar.

\end{itemize}

DVR methods are especially appropriate for creating images from datasets containing amorphous features like clouds, fluids, and gases. One disadvantage of using DVR methods
is that the entire dataset must be traversed each time an image is
rendered. A low-resolution pass or random sampling of the data is
sometimes used to create low-quality images quickly for parameter
checking. The process of successively increasing the resolution
and quality of a DVR image over time is called progressive
refinement.

 The user begins by choosing a threshold value
and then geometric primitives are automatically fit to the high-contrast
contours in the volume that match the threshold. Cells
whose comer-values are all above the chosen threshold (cell is
inside) or all below the threshold (cell is outside) are discarded and
have no effect on the final image. Showing just the cells falling on
the threshold is sometimes useful, but can be a problem. Another
consideration is the huge number of surface primitives generated
for large volumetric datasets. 


Many steps in the volume visualization process are common to
volume visualization algorithms.  Most of the fundamental volume
visualization algorithms include only a subset of the steps listed
here.
The initial step in every procedure is data acquisition. The next common step is to pat the data slices into a form that can be worked with and then to process each slice so that it covers a good distribution of values, is high in contrast, and is free of noise and
out-of-range values. Of course, the same set of processes must be
applied to every slice.
Next, the dataset is reconstructed so that the ratio of the
dimensions is proportional to the ratio of the dimensions of the
measured substance or substance being simulated. This may
involve interpolating between values in adjacent slices to construct
new slices, replicating existing slices, interpolating to estimate
missing values, or scan-converting an irregular grid or non-orthogonal
grid onto a Cartesian grid by interpolation. At this point, a three-dimensional enhancement may be applied, such as a slight
blurring of the data values. Next, a data-classification or
thresholding is performed. This step will be discussed in detail in
the section on data classification.
After data-classification, a mapping operation is applied to map
the elements into geometric or display primitives. This is the stage
that varies the most between algorithms, as will be shown in the
section on volume visualization algorithms. At this point, the
primitives can be stored, manipulated, intermixed with externally
defined primitives, shaded, transformed to screen space, and
displayed. The shading and transformation steps can be reordered
and shading can be done in one of eleven ways as explained in 


\subsection{Direct Volume Rendering}

\subsubsection{Texture mapping}

\paragraph{2D Texture mapping}
2D Texture mapping consists in representing the whole volume as a set of slices parallel to coordinate planes. To create images of the slices, one can use the standard texture mapping capabilities provided by graphics libraries like OpenGL. \ref{fig:2dslices} shows how pixel colors and opacities are calculated when rendering a texture-mapped polygon. In texture mapping, the texture coordinates (s, t) are stored along with each vertex. These coordinates are interpolated across the polygon during scan conversion and are used as coordinates for color look-up inside a texture image \ref{fig:2dtexture}. Color and opacity values from the texture image are reconstructed using bi-linear interpolation.


\begin{figure}[th]
\centering
\includegraphics{Figures/2D_slices}
\decoRule
\caption[2D Sclices]{2D slices of the volume.}
\label{fig:2dslices}
\end{figure}
On modern graphics workstations, the complete process of interpolating texture coordinates and reconstructing texture values is incorporated into the scan-conversion process and done completely in hardware. Because texture mapping is such a common application, these routines are well-optimized and highly efficient.
To simulate ray casting using texture mapping, we have to slice the volumetric dataset into parallel slices, use these slices as texture images to texture-map the projections of the sliced onto the image plane. To composite the slices, we can use the blending operations provided by the OpenGL graphics library. As it happens, one of the standard operators provided is exactly the compositing operator used in ray casting. Since compositing is also fully hardware-assisted, it is also very fast.
\begin{figure}[th]
\centering
\includegraphics{Figures/2dtexmapping}
\decoRule
\caption[2D texture mapping]{Calculating color and opacity of a pixel inside a texture-mapped polygon.}
\label{fig:2dtexture}
\end{figure}

There are two ways to deal with varying viewpoints.

The first one is to create slice images for the given data set in a pre-processing step and uses different projection matrices to create arbitrary-viewpoint images of these slices. This is fast since the (expensive) step of slice creation has to be done only once. On the other hand, image quality suffers because the views of all slices will be distorted due to the projection onto the image plane. In the extreme case of the view direction being parallel to the slices, there will be no image at all.

The second way is to create slices which are always orthogonal to the viewing direction. This results in optimal image quality, but it means that the slice stack has to be re-generated for each new image. Furthermore, arbitrary ("oblique") slices through a cuboid are generally not square or rectangular and can be anything from triangles to hexagons.

\begin{algorithm} \caption{Volume Rendering using 2D Texture Mapping Algorithm} 
\label{alg:tex2dmapping}
\begin{algorithmic}[1]
\Require $N$ the number of slices, $\vec{v} $ the view direction
\State Load the data $\mathcal V(x,y,z) \gets data$ 
\State create the slices $s_i \in \mathcal V$ with each normal $\vec{n_i} \parallel \vec{v}$
\State Turn off the Depth test 
\State Enable the Blending 

\ForEach {$s_i \in \mathcal V $ form back to front}
\State $Texture \gets s_i$
\State create a polygon $p$ corresponding to the slice $s_i$
\State Assign texture coordinates to four corners of the polygon
\State Render and blend the polygon (alpha blending) 
\EndFor

\end{algorithmic}
\end{algorithm}

\paragraph{3D Texture mapping}
To allow the interactive generation of view-orthogonal slices, a special hardware technique has been developed. This is a generalization of texture-mapping to three-dimensional textures, appropriately called "3D texturing."
As seen in Figure \ref{fig:2dtexture}, 2D texture-mapping interpolates two additional coordinates (s, t) across a polygon's interior. In 3D texture-mapping, three additional coordinates (s, t, r) are interpolated. To determine a pixel's color and opacity, these three coordinates are used as indices into a three-dimensional image, the 3D texture, see Figure \ref{fig:3dtexture}. To reconstruct texture values, tri-linear interpolation is used.

3D textures allow direct treatment of volumetric data. Instead of generating a set of two-dimensional slices in a pre-processing step, the volumetric data is directly downloaded into the graphics hardware and is directly used to calculate color and opacity values for each pixel covered by a rendered primitive.

\begin{figure}[th]
\centering
\includegraphics{Figures/3dtexmapping}
\decoRule
\caption[3D texture mapping]{Calculating color and opacity of a pixel inside a texture-mapped polygon using 3D texture.}
\label{fig:3dtexture}
\end{figure}


To generate oblique slices using 3D texturing, one only has to calculate the vertices of the slice and then to generate the correct 3D texture coordinates for those vertices, see Figure \ref{fig:cubeSlice}. Then these coordinates are passed to the graphics library, and the graphics library will render the projection of the slice onto the image plane, using tri-linear interpolation to reconstruct each pixel's color and opacity values.

\begin{figure}[th]
\centering
\includegraphics{Figures/cubeSlice}
\decoRule
\caption[3D Cube slice]{Calculating vertex coordinates and texture coordinates for an oblique slice. In this figure, the vertex and texture coordinates are identical.}
\label{fig:cubeSlice}
\end{figure}

\subsubsection{Raycasting}
Volume raycasting computes a 2D image from the 3D volumetric data set. The basic goal of ray casting is to allow the best use of the three-dimensional data and not attempt to impose any geometric structure on it. It solves one of the most important limitations of surface extraction techniques, namely the way in which they display a projection of a thin shell in the acquisition space, see \cite{Stegmaier:2005:SFV:2386472.2386498}. Surface extraction techniques fail to take into account that, particularly in medical imaging, data may originate from fluid and other materials which may be partially transparent and should be modeled as such. Ray casting doesn't suffer from this limitation. This algorithm can be split into four main steps (see Figure \ref{fig:rayasting}) :
\begin{itemize}

\item \textbf{Ray casting}. For each pixel of the final image, a ray of sight is shot ("cast") through the volume. At this stage, it is useful to consider the volume being touched and enclosed within a bounding primitive, a simple geometric object (usually a cuboid) that is used to intersect the ray of sight and the volume.

\item \textbf{Sampling}. Along the part of the ray of sight that lies within the volume, equidistant sampling points or samples are selected. In general, the volume is not aligned with the ray of sight, and sampling points will usually be located in between voxels. Because of that, it is necessary to interpolate the values of the samples from its surrounding voxels (commonly using trilinear interpolation).

\item \textbf{Shading}. For each sampling point, a transfer function retrieves an RGBA material color and a gradient of illumination values is computed. The gradient represents the orientation of local surfaces within the volume. The samples are then shaded (i.e. colored and lit) according to their surface orientation and the location of the light source in the scene.

\item \textbf{Compositing}. After all sampling points have been shaded, they are composited along the ray of sight, resulting in the final color value for the pixel that is currently being processed. The composition is derived directly from the rendering equation and is similar to blending acetate sheets on an overhead projector. It may work back-to-front, i.e. computation starts with the sample farthest from the viewer and ends with the one nearest to the viewer. This workflow direction ensures that masked parts of the volume do not affect the resulting pixel. The front-to-back order could be more computationally efficient since, the residual ray energy is getting down while ray travels away from camera; so, the contribution to the rendering integral is diminishing, therefore, more aggressive speed/quality compromise may be applied (increasing of distances between samples along ray is one of such speed/quality trade-offs).

\end{itemize}

\begin{figure}[th]
\centering
\includegraphics[width=\textwidth]{Figures/raycasting}
\decoRule
\caption[Volume raycasting]{ The four steps of the volume raycasting algorithm  }
\label{fig:rayasting}
\end{figure}

\subsection{Surface-fitting}

This subsection describes an algorithm for creating a polygonal surface representation of an isosurface of a 3D scalar field. A common name for this type of problem is the so-called \textbf{ "marching cubes" } algorithm. It combines simplicity with high speed since it works almost entirely on lookup tables.

The fundamental problem is to form a facet approximation to an isosurface through a scalar field sampled on a rectangular 3D grid. Given one grid cell defined by its vertices and scalar values at each vertex, it is necessary to create planar facets that best represent the isosurface through that grid cell. The isosurface may not be pass through the grid cell, it may cut off any one of the vertices, or it may pass through in any one of a number of more complicated ways. Each possibility will be characterized by the number of vertices that have values above or below the isosurface. If one vertex is above the isosurface say and an adjacent vertex is below the isosurface then we know the isosurface cuts the edge between these two vertices. The position that it cuts the edge will be linearly interpolated, the ratio of the length between the two vertices will be the same as the ratio of the isosurface value to the values at the vertices of the grid cell.


There are two major components of the Marching cubes algorithm proposed by \cite{Lorensen:1987:MCA}. The first is deciding how to define the section or sections of the surface which chop up an individual cube. If we classify each corner as either being below or above the isovalue, there are 256 possible configurations of corner classifications. Two of these are trivial; where all points are inside or outside the cube does not contribute to the isosurface. For all other configurations we need to determine where, along with each cube edge, the isosurface crosses, and use these edge intersection points to create one or more triangular patches for the isosurface.


If you account for symmetries, there are actually only 14 unique configurations in the remaining 254 possibilities. When there is only one corner less than the isovalue, this forms a single triangle which intersects the edges which meet at this corner, with the patch normal facing away from the corner. Obviously, there are 8 related configurations of this sort. By reversing the normal we get 8 configurations which have 7 corners less than the isovalue. We don't consider these really unique, however. For configurations with 2 corners less than the isovalue, there are 3 unique configurations, depending on whether the corners belong to the same edge, belong the same face of the cube, or are diagonally positioned relative to each other. For configurations with 3 corners less than the isovalue there are again 3 unique configurations (e.g. for configuration 14), depending on whether there are 0, 1, or 2 shared edges (2 shared edges gives you an 'L' shape). There are 7 unique configurations when you have 4 corners less than the isovalue, depending on whether there are 0, 2, 3 (3 variants on this one), or 4 shared edges (e.g. for configuration 30 - again you may need to tweak the colors to see the triangle for the isolated (far) inside sphere/pixel).

Each of the non-trivial configurations results in between 1 and 4 triangles being added to the isosurface. The actual vertices themselves can be computed by interpolation along edges, or, default their location to the middle of the edge. The interpolated locations will obviously give you better shading calculations and smoother surfaces.

The quality of the final the polygonal surface representation heavily depends on the number on isosurface generated. According to the implementation used and the optimization strategies we can find a better trade-off between the quality and the frame rate (see \autoref{fig:isosurface}). 


Using isosurfaces allow to be faster than direct volume rendering. However, we lose the flexibility and the precision of the DVR algorithms because of the different approximations made while extracting these isosurfaces according to a maximum number of surfaces. 


\begin{figure}
\centering
\includegraphics[width=\textwidth]{Figures/isosurface}
\caption[Volume raycasting]{ Two  molecules, generated at different grid sizes }
\label{fig:isosurface}
\end{figure}

\subsection{summary}
The existing algorithms to render volumetric data-sets can be categorized into two major types: Direct Volume Rendering (DVR), and Surface-fitting algorithms.


Rendering a volume directly with DVR methods is usually better for visualizing softer things like flows fields and density variations, or to take advantage of the transparency. Algorithms like raycasting allow to provide results of very high quality, usually considered to provide the best image quality. 


Isosurfaces from surface-fitting methods tend to be a popular form of visualization for volume datasets since they can be rendered by a simple polygonal model, which can be drawn on the screen very quickly. They are very useful when we want to examine structures and boundaries in volume data, such as bone shape or distinct parts of the brain. It is then better to extract a triangle or tetrahedral mesh from the voxel data or implicit field. These meshes can be studied for their topology and curvature, and rendered as solid looking objects.



\section{Occlusion management strategies}



\subsection{Transfer Function}

A transfer function (TF) maps volumetric data to optical properties
and is part of the traditional visualization pipeline: data acquisition,
processing, visual mapping, and rendering. Volumetric data is
considered to be a scalar function from a three-dimensional spatial
domain with a one-dimensional range (e.g., density, flow magnitude,
etc.). Image generation involves mapping data samples through the
TF, where they are given optical properties such as color and opacity,
and compositing them into the image.
A TF simultaneously defines which parts of the data are essential
to depict and how to depict these, often small, portions
of the volumetric data. Considering the first step, a TF is a special,
but important, case of a segmentation or classification. With classification, certain regions in a three-dimensional domain are identified
to belong to the same material, such as bone, vessel, or soft tissue, in medical imaging. A plethora of classification and segmentation
algorithms have been developed over the last decades, with semiautomatic
approaches often tailored to specific application scenarios.
Segmentation algorithms can be quite intricate since information
from the three-dimensional spatial domain and the one-dimensional
data range are taken into account. TFs in their basic form are, on the
other hand, restricted to using only the data ranges. In comparison
with general classification algorithms, this characteristic makes a
TF less powerful with respect to identifying relevant parts of the
data. The advantage of a TF is, however, a substantial performance
gain as classification based on the one-dimensional data range reduces
the complexity tremendously. This gain is the result of the
three-dimensional domain being typically two orders of magnitude
larger than the small data range. Histograms are an example of discarding
potentially complex spatial information and aggregating
only the data values to binned-frequency information. In the same
spirit, TFs classify interesting parts of the data by considering data
values alone. The second functionality of a TF deals with specifying
optical properties for portions of the data range previously identified
as being relevant

The transfer functions can be classified according to different criteria. Some of the most important are:

\begin{itemize}
\item \textbf{The dimension: } The one-dimensional TF classifies the scalar data value, $d$, and subsequently maps the material to an optical property for rendering: $ \textbf{q}(d)=\textbf{V}(\textbf{M}(d))$ where $\textbf{M}(.)$ is the material classification function and $\textbf{V}(.)$ is the
visual mapping of the material. One-dimensional TFs are adequate in many cases of simulation data where measurement noise is low or even non-existent where different materials of interest have few overlapping intensity
ranges \cite{4303986}. One-dimensional TFs are often the first tool available in software packages providing volume rendering, as they are relatively easy to comprehend for the novice or occasional user. Practically all production
visualization software, such as ParaView by \cite{paraview},
 VisIt by \cite{Childs:SciDAC2011}, 
or ImageVis3D by {\cite{Fogal2010Tuvok}}, support 1D TF editors.

\begin{figure}[th]
\centering
\includegraphics[width=0.7\textwidth]{Figures/2dtf}
\decoRule
\caption[2D transfer Function]{ The two-dimensional TF editor widgets allow the user
to more precisely control the classification power of the 2D TF.
The triangular widget can also be skewed to better fit the desired
classification domain (\cite{1021579}) }
\label{fig:tf2d}
\end{figure}

Multidimensional TFs are used for Multidimensional data. If the user has to manipulate the TF definition directly, moving beyond
2D TFs immediately poses significant challenges in terms of
user interfaces and cognitive comprehension (\cite{1021579}). Much research and
work on Multidimensional TFs are, therefore, related to various forms of automation
and advanced user interfaces. Typical approaches include dimensional reduction, clustering
and grouping, machine learning, and various user interface
approaches such as parallel coordinates by \cite{5742368} or direct slice or volume
view interaction. 

\begin{figure}[th]
\centering
\includegraphics[width=\textwidth]{Figures/mdtf}
\decoRule
\caption[Multidimensional transfer Function]{ Parallel coordinates with embedded multidimensional scale plots for editing multidimensional TFs(\cite{5742368}) }
\label{fig:tfmd}
\end{figure}


\item \textbf{The automation}: The automation includes techniques such as adapting presets, semi-automatic, automatic, and supervised machine learning. Volume rendering has established itself as
a powerful visualization tool in many domains, but the necessary
design of TFs is a time consuming and tedious task. Consequently,
a significant amount of work is dedicated to the automatic and semiautomatic generation of TFs. 

\item \textbf{The aggregated attributes}: Conceptually, the introduction of additional quantities in the TF definition makes it possible to discriminate more parts in a dataset. Histogram clustering helps to reduce the degrees of freedom in designing a TF. As an illustration, \cite{5887324} propose modeling the
histogram space using a Gaussian mixture model. An elliptical TF
is assigned to each Gaussian, and the user interaction is simplified in order to edit the parameters of these ellipsoids.  

\end{itemize}

\subsection{Segmentation}

Many segmentation algorithms allow extracting features from images (2D) and 3D volumes. In this subsection, we present two common algorithms: \textbf{Thresholding}, and \textbf{Clustering}

\subsubsection{ Thresholding }
Scalar images can be segmented using thresholding approaches by partitioning the volume intensities. This methodology attempts to determine an intensity value that can separate the signal into a desired number of classes. Segmented volumes can be achieved by clustering all pixels with intensities larger than the threshold value into one class, and all others into another. In many applications, the threshold values selection can be done depending on the basis of a histogram. Multithresholding occurs when more than one threshold value is determined (see \cite{Sahoo:1988:STT:46072.46079}).
The voxels of a certain object are not necessarily connected after thresholding because this technique does not consider the spatial characteristics of an image, thus causing it to be sensitive to noise and intensity fluctuations. For this reason, it cannot be easily applied to many medical imaging modalities. These drawbacks essentially corrupt the histogram of the image-making partitioning via the selection of more problematic appropriate thresholds.

3D thresholding method is similar to the 2D approaches where thresholding process is applied to all pixels in the volume instead of that in the plane.


\subsubsection{ Clustering }
Clustering technique is the process of classifying each group of pixels in an image into one class, each class has the same or similar properties which evaluate a specific part of an image or a volume. Each class is highlighted in the segmented volume to illustrate the volume as a number of separated regions, and the region of interest may be one of those regions. Clustering technique is based on multi-threshold values which can be set depending on the image histogram. \cite{AMIRA20081954} propose a method for segmenting medical volumes; it is the most commonly used clustering technique.
K-means clustering classifies n voxels into K clusters or classes ( K less than n ). This algorithm chooses the number of clusters (K) then randomly generates K clusters and determine the cluster centers. The next step is assigning each point in the volume to the nearest cluster center and finally recompute the new cluster centers. The two previous steps are repeated until the minimum variance criterion is achieved. This approach is similar to the expectation-maximization algorithm for Gaussian mixture in which they both attempt to find the centers of clusters in the volume.

\cite{Li:2012:LVV:2425296.2425325} present a system for luggage visualization where any object is
clearly distinguishable from its neighbors. It supports virtual unpacking
by visually moving any object away from its original pose. To virtually unpack a luggage, instead of physically partitioning the
original volume, the system performs volume rendering in multiple passes
and each pass is associated with the proper transfer function and geometric
transformation ( see \autoref{fig:unpack} ).  A brutal-force design requires the number
of rendering passes equals to that of the objects in the worst case.
This obviously is too expensive considering a typical luggage contains
several dozens of objects. To efficiently handle a scene mixed
with a volume of packed objects, unpacked objects, and the animation
of objects being unpacked or restored, they have designed a layered
approach. Each layer is responsible for rendering objects in a
particular state as well as caches and displays the previously rendered
images to avoid unnecessary rendering. The layer of unpacked objects
is further decomposed into a set of 3D sprites to minimize
memory usage.

\subsection{Deformations and Focus + Context}

%%camera !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Almost all images used in visualization are computed with a planar pinhole camera (PPC). One reason for this is that the PPC models the
human eye well, producing images that resemble what users would
see if they were actually looking at the dataset to be visualized.
Another reason is simplicity: software and hardware
implementations of PPC rendering algorithms allow visualizing
complex 3-D datasets at interactive rates. However, the simplicity of
the PPC model also brings three important limitations. First, the PPC
has a limited field of view. Second, the sampling rate of the PPC is
uniform over its entire field of view. Third, the PPC has only a single
viewpoint, i.e. the pinhole where all rays converge.
\newline \textbf{Multiperspective }  visualization is a promising approach based on
integrating data sampled from multiple viewpoints into a single
image. The multiple viewpoints are integrated tightly which
alleviates the visualization discontinuity problem of multiple
individual images. Like in the case of the dataset distortion approach,
multiperspective visualization amounts to a warp of global spatial
relationships between data subsets. However, multiperspective
visualization allows specifying the desired disocclusion effect
directly in the image, as opposed to indirectly, through a dataset
distortion. Finally, multiperspective visualization does not preclude
but rather enhances interactive exploration of datasets. The
multiperspective image provides a preview of data subsets to come
which improves interactive visualization efficiency. Multiperspective visualization has challenges of its own. The
multiperspective image is computed using a non-pinhole camera
model that does not project 3-D lines to image plane lines, so one
challenge is achieving the desired disocclusion effect while
minimizing visualization distortions. The non-pinhole camera model
is considerably more complex than the PPC model, so a second
challenge is to achieve adequate rendering performance to support
interactive visualization and dynamic datasets. Eliminating the single
viewpoint constraint of the PPC model results in a multidimensional
camera model design space. Whereas for the PPC model the only
intrinsic parameter of significant relevance in shaping the
visualization is the focal length, optimizing multiperspective
visualization requires tuning a large number of parameters.
Consequently, a third challenge is to specify the camera model that
best visualizes a given dataset from a given location. \newline
\cite{5613463} proposed a multiperspective visualization technique
based on the curved ray camera (CRC). The CRC is designed to
address the challenges of multiperspective visualization described
above. The CRC integrates multiple viewpoints seamlessly. The
curved rays allow for a progressive transition between one viewpoint
and the next. A CRC ray is a sequence of line segments connected by
conic curve segments. Each conic connects consecutive line
segments with C'
 continuity, which alleviates visualization
distortions. The CRC provides a fast projection operation which
allows rendering 3-D surface datasets efficiently by projection
followed by rasterization, with the help of graphics hardware. The
rays of the CRC can be traced inexpensively which enables
visualization techniques that require ray casting, such as volume
rendering. \newline
\cite{7120994} extended this work by proposing a framework that allows constructing continuous multiperspective visualizations
by changing the viewpoint for individual focus regions
in an image, by connecting input images with continuous
context, and by alleviating occlusions to moving targets
without distorting or displacing the target sub-images.

An interactive lens is a lightweight tool to solve localized visualization problems by temporarily altering a selected part of the visual representation of the data~\cite{CGF:CGF12871}. Using this lens approach, we propose an interactive volume deformation based on GPU accelerated ray-casting to free a designated target from local occlusion while keeping the global context.

A lens is a parameterizable selection according to which a base visualization is altered. Typically, a lens is added to a visualization to interactively solve a specific localized problem. This property is very interesting with the aim of providing a focus+context solution to occlusion in volume rendering. Lenses can have different geometric properties not only defining their appearance, but also determining the selection, which is the subset of the data where these lenses take effect. The major geometric properties of a lens are shape, position, and size as well as the orientation.
The shape of a lens is usually chosen to fulfill the requirements of the application and is strongly linked to the lens function. Most virtual lenses are circular~\cite{1648236} or are rectangular~\cite{Kincaid:2010:SFA:1907651.1907963} as the real-world ones (magnifying glass, windows). Our lens has also a circular shape in order to remind its magnifying property. Some lenses, such as the  JellyLens~\cite{Pindat:2012:JCA:2380116.2380150} and the smart lenses~\cite{Thiede2008} can adapt their shape automatically according to the data. 
The position and the size parametrization can increase the flexibility of an interactive lens.
Modifying this position or size will set its focus on a different part of the data according to the user's interest. It is possible to update automatically these parameters in order to guide the user toward interesting events in the data~\cite{Tominski:2011:ECU:2336207.2336211}, or adjust the lens position according to predefined paths as the Routelens~\cite{Alvina:2014:RER:2598153.2598200} does. With this in mind, our lens updates automatically its properties once a target has been selected. This allows a smooth transition towards an unobstructed and magnified area of interest. 

Lenses for volume visualization face challenges mainly related to spatial selection and occlusion. \cite{1532818} addressed these issues by proposing the Magic Lens. This framework proposes FOUR different types of lenses allowing a free-form volumetric lens function that
can be feature-adaptive or user-configurable for a high-quality
aliased, and interactive display with smooth transitions from high to
low-resolution areas.


In addition to interactively magnifying areas and objects of interest, our lens frees them from obstruction and allows local modification of the camera to see the target under other perspectives. \cite{7539643} proposed the GlyphLens that removes the occluding glyphs by pulling the glyphs aside through the animation, but this tool is only well suited for systems where 3D volumetric dataset are visualized using glyphs. Lenses can create discontinuities between their inner part and the rest of the volume. Deformation can be a solution to this discontinuity issue.   




 \cite{Hsu:2011:RFM:2070781.2024165} developed a framework that can generate non-linear sampling rays that smoothly project objects in a scene at multiple levels of detail onto a single image. Such a technique requires a lot of computational time to render a single image from features of interest at different scales, see figure \ref{fig:multiscale}. This multiscale rendering framework consists of a sequence of pinhole cameras, each of which captures a view of interest at a particular scale. The camera rays for each pixel in the final projected image are generated based on a user-defined image mask which specifies the interesting regions in each view.  The camera rays non-linearly cast through all scales of interest. Since the camera rays in the model are bent coherently and march consistently, the objects projected on the image are continuous in both image-space and object-space. 
 Their method starts by setting up several pinhole cameras for viewing different scales of interest, utilizing most users’ ease and familiarity with manipulating ordinary pinhole cameras. Each camera produces an image of its view. In order to combine all such views to form a single multiscale image, a user-specified image mask is used to indicate regions of each view that the user would like to show in the final image. In other words, every camera projects only part of its view onto the final image space, based on a corresponding image mask. In order to ensure consistency while projecting different camera
viewpoints onto different parts of the image, we force all camera rays to originate from the first camera, which is the one that has the largest scale of view. The rest of the cameras define intermediate
points that camera rays must pass through in order, from the largest to smallest scales. They use B\'ezier curves to connect the near-clipping planes of two successive cameras.

  The rays are bent gradually from one scale to another to maintain object-space continuity as well as image-space smoothness. The multiscale camera can also achieve a focus+context effect, a technique frequently used in many visualization applications. Finally, we show that it can potentially create pictures that mimic artistic or impossible views of objects or scenes like the kind made by the artist M. C. Escher. 
 
\begin{figure}[th]
\centering
\includegraphics[width=\textwidth]{Figures/multiscale3d}
\decoRule
\caption[ multiscale ]{ A Rendering Framework for Multiscale Views of 3D Models. A continuous multiscale view (right) of a volumetric human body dataset shows three different levels of detail (left three) in a single image. The image on the right is directly rendered with our multiscale framework.}
\label{fig:multiscale}
\end{figure} 
 
  Bruckner and  Groller~\cite{4015467} proposed exploded view for volume data by partitioning the volume into several segments (figure \ref{fig:exploded}). Basically, in an exploded view the object is decomposed into several parts which are displaced so that internal details are visible. This does not only give an unobstructed view on the focus but also potentially reveals other interesting information, such as
cross-sections of the split object. The advantage of exploded views is that they simultaneously convey the global structure of the depicted object, the details of individual components, and the local relationships
among them. A Force-directed layout approach is used to arrange three-dimensional objects
in such a way that they do not occlude another object, but with as little displacement as possible. Four forces are used. \textbf{The Return force } is an attractive force that tries to move the parts towards
their original location. The force $F_r$ is realized as a logarithmic spring:
 \[ F_r=c_{r}\ln(||r||)\cdot \frac{r}{||r||} \] 
 where $r$ is the vector from the vertex's current position to its original location and $c_r$ is a constant factor.
 \textbf{The Explosion force} drives the specified parts away from our selection object. The explosion point is also weighted according to the size of the region corresponding to an octree node. Each explosion point exerts a force $F_e$ on every part $P_i$:
  \[ F_e=  \frac{c_e}{e^{||r||} } \cdot \frac{r}{||r||} \]
  where $r$ is the vector from the explosion point to the closest point of the part geometry of $P_i$ and $c_e$ is a scaling factor.
   \textbf{The Viewing force} is a view-dependent force which attempts to arrange parts so that they do not occlude the selection for the current viewing transformation. The force $F_v$ is:
   \[ F_v=  \frac{c_v}{||r||} \cdot \frac{r}{||r||} \] 
   where $r$ is the vector from the closest point along the viewing ray to the center of the body and $c_v$ is a scaling factor.
   \textbf{The Spacing force} prevents clustering of parts, we also add a
repulsive force $F_s$. For a part $P_i$, the spacing force exerted by another part $P_j$ is:
\[ F_s=  \frac{c_s}{||r||^2} \cdot \frac{r}{||r||} \] 
where $r$ is the vector from the center of $P_j$ to the center of $P_i$ and
$c_s$ is a constant scaling factor. 
\begin{figure}
\centering
\includegraphics[width=\textwidth]{Figures/explodedview}
\decoRule
\caption[ Interactive exploded-view]{ Interactive exploded-view illustration of a human head with increasing degrees-of-explosion. Two hinge joints are used to constrain part
movement}
\label{fig:exploded}
\end{figure}

  
\cite{4015450} proposed ClearView, an interactive and intuitive volume
visualization tool that provides the user with a simple exploration
metaphor, see figure \ref{fig:clearview}. Following traditional technical illustrations, several 3D
layers of volumetric data sets are extracted using texture-based raycasting.
They are composed to produce high-quality images at high
frame rates. The user guides the exploration process by moving the
hotspot, a lens-like yet distortion free region, in which additional layers
of the data set are augmented to convey relevant features. The proposed
GPU system exploits feature-based techniques to improve the
understanding of complex 3D data sets, and it utilizes image-based
deferred shading to maximize performance. ClearView does not require
any additional feature volumes, making the method suitable for
the interactive rendering of high-resolution data sets on recent GPUs.
Several shaders to intuitively convey material and shape properties are
integrated into the system. To keep the user interface slim, only a few
parameters abstract from the technical realization. The user simply
positions the hotspot on the data set and selects an amount of transparency
to continuously blend between focus and context information.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{Figures/clearview}
\decoRule
\caption[ clearview]{ The ClearView system can be used to visually explore complex data sets at interactive frame rates by using a focus+context metaphor. Even the leftmost image
of the 5123 visible human datasets with multiple iso-surfaces was generated at about 15 fps on an 800x600 viewport.}
\label{fig:clearview}
\end{figure}

  
  \cite{1250400} performed deformations using peeling to see hidden parts of the data. Their goal in using deformations is to increase the visibility of
the inner portions of the volume, without completely removing the
surrounding data that normally occludes the inside. This is akin to
focus+context schemes that allow a user to zoom in on data of
interest while using remaining screen space to show the surrounding
context. Appropriately chosen deformations could, for example,
split open a volume, showing displaced structures side-by-side,
making it easy for the user to see how they connect and allowing
the user to mentally stitch them together into a whole. Deformations
with familiar real-world analogs (e.g. cutting and peeling the skin off a fruit, or the layers of an onion) are also likely to be
readily understood by users.
Therefore in their work, they describe a prototype system that implements
different metaphors for deformation-based browsing of volumetric
data. 
As will be seen, a key element of our approach is to support
differential treatment of the various semantic layers in a data set.
The term ''semantic layers'', means subsets of the data that are useful
or meaningful to the user. These layers could be defined geometrically,
for example as sections created with parallel planar cuts.
More typically, layers would depend on the voxel data values, for
example boundaries that are found during segmentation or isosurface
extraction. In the context of medical visualization, there is at
least anecdotal evidence that anatomists, for example, prefer to remove
tissue layer by layer, rather than making
arbitrary planar cuts.
 
 
Deformations can reveal predefined features in the dataset by taking into account the precomputed segmentation. Tong et al. proposed a deforming Lens which moves streamlines to observe the inner part of streamline bundles~\cite{7332955}. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{Figures/cut}
\decoRule
\caption[ Feature Aligned Volume Manipulation]{ A feature-aligned retraction applied to a human hand data set, showing bones (left) and vessels (right)}
\label{fig:cut}
\end{figure}
\cite{Correa:2007:IDD:1313046.1313163} proposed a framework  allowing the users to physically or actively manipulate the geometry of a data object. Some studies performed deformations using surgical metaphors like ~\cite{4069230,Correa:2006:FAV:1187627.1187827} to see hidden parts of the volume, see figure \ref{fig:cut}. They propose a feature-based technique for manipulating volumetric objects for illustrative visualization, and describe a GPU-based implementation that enables interactive specification and their rendering.
Inspired by medical illustrations that frequently
depict the results of manipulation with tools such as peelers,
retractors and pliers, this system allows the specification of manipulation
through a collection of procedurally-defined manipulation operators.



\begin{figure}
\centering
\includegraphics[width=\textwidth]{Figures/peel.pdf}
\decoRule
\caption[ Feature Aligned Volume Manipulation]{ An example of different types of alignment. (a) Axis-aligned peel. Note how the peeled layer is thick and flat since it is aligned with
an orthogonal axis. (b) Surface aligned peel, aligned with a computed
distance field. Notice how it approximates a surface. (c) Segment
aligned peel, based on segmentation, which is more accurate. Note
that in the feature based alignment (b) and (c) the peel is thin and
rounded.}
\label{fig:peel}
\end{figure}

 Correa et al. introduced two feature-based methods, namely surface alignment and segment alignment for modeling and rendering illustrative manipulation, and compare them with the traditional axis alignment method, see figure \ref{fig:peel}. They devise a method for estimating accurate normals along the surface of cuts
and dissections while maintaining continuous normals, which allows
us to obtain correct shading of the object being manipulated, opaque
or translucent. The four operators proposed in this framework are the following:
\begin{itemize}
\item Peeler: The peeler simulates peeling or cutting of the outer layers
of a volumetric model. Different parameters
of the peeler, such as the thickness of the cut and the angle of bending of
the peeled layer, can be manipulated interactively to obtain different
illustrative effects.
\item Retractors: In the context of surgical illustration, retractors are tools used to spread organs or bones, or to hold back soft tissues such as
skin. In the context of illustration, they are useful for illustrating the
access to the internal organs.
\item Pliers: Pliers are tools used to grasp tissue and pull it or poke it into the volumetric object. The operating parameters include the shape of
the displacement and the radius of influence which specifies how the
displacement propagates through the volume.
\item Dilator: Dilators are used to gain access into narrow regions, cuts
or vessels. They essentially scale the region from the inside typically
by blowing air, to increase the visibility or accessibility of the region.
When applied to volumetric manipulation, dilators have a similar effect
to that of volumetric magic lenses (\cite{1532818}).
\end{itemize} 
However, this framework does not offer tools for local manipulation and exploration inside the cut, which allows perceiving a target from a different perspective while keeping the global context. 


\cite{6787171} propose a set of linked views that display
subsets of data attributes. Example views are 3D DVR plots, 2D
scatterplots, and 2D and 3D histograms. Views are linked by brushing
and free-form selection. Using the optimal view(s), one can find structures
of interest, e.g. spatially compact zones in DVR renderings or peaks in histograms, and highlight or erase such structures in all views
at once. They enhance classical histogram views with shading, sorting,
and depth, to allow spotting complex patterns with greater ease. In addition, they
propose a smooth animation between multiple views, to locate (and select)
data patterns which are hard to isolate in static plots. This framework integrates
a Focus+Context deformation that adds the ability to uncover locally occluded
spatial patterns in different views. The GPU implementation of their
techniques, which they call color tunneling, creates real-time interactive,
animated, explorations of datasets of tens of millions of points
on a modern PC. Since the goal is to explore dense multivariate datasets which exhibit significant
overlap between groups of voxels or pixels, tools to
interactively unveil the occluded structures of interest are needed.
Five interactive techniques have been proposed as follows (see figure \ref{fig:colortunneling}):
\begin{itemize}
\item View linking: Three configurable views (2 exploration views and one lock view) for data exploration,
\item Warp: Animates a view between two configurations,
\item Lock: Locks items not to be affected by brush or dig,
\item Brush: With the lock view, brushing allows adding or removing
data in a view,
\item Dig: With the lock view, digging pushes data points away from
the lens center to unveil occluded structures 

\end{itemize}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{Figures/colortunneling}
\decoRule
\caption[ Color Tunneling]{ Brush, dig, and lock tools. Locked items are not affected by
brush and dig. We used transfer functions to make air voxels transparent
and standard gradient shading. We see that the head is surrounded by a
large amount of uninteresting noise (yellow).}
\label{fig:colortunneling}
\end{figure}



\cite{Li:2012:LVV:2425296.2425325} present a system for luggage visualization where any object is
clearly distinguishable from its neighbors. It supports virtual unpacking
by visually moving any object away from its original pose. To achieve these, we first apply a volume segmentation guided by
a confidence measure that recursively splits connected regions until
semantically meaningful objects are obtained, and a labeled volume
whose voxels specifying the object IDs are generated. 

First, a volumetric representation of a luggage is
segmented into meaningful real-world objects. Then the segmented
objects are assigned distinguishable colors depending on the viewing
parameters. The original luggage volume, the segmentation
results, and the automatically assigned colors are fed into a volume
rendering system. For any user-defined unpacking direction,
unpackable objects are determined and unpacking is visualized by
showing that the transition of the objects flying from its original
position to an unpacking destination. Unpacked objects can be manipulated
individually, such as rotating, changing transfer function,
etc, for inspection. The system keeps packed luggage, already unpacked
objects, and the objects in transition in the same scene and
allows repacking.

For the luggage segmentation strategy, this system utilizes a the
graph partitioning approach by \cite{1580491} enhanced
by a confidence measure of the quality of a segmentation. The
confidence measure gives high scores to semantic meaningful real-world
objects that are homogeneous and have well-defined boundaries
against the surrounding segments, and it guides the segmentation algorithm to recursively split connected objects in an input
volume until the segmentation quality cannot be improved any more.

In order to assign distinct colors to objects close to each other in
an image, their system first generates a layered image of object IDs,
from which whether any pair of objects is interfering with each other
is determined. A modified graph coloring algorithm is then computed
to ensure any interfering pair are assigned distinguishable
hues while the hue space is fully exploited. In order to reduce volume rendering cost as well as to keep smooth transitions across
touching objects, this system adopts a tinting approach, in which the assigned
hues of an object is modulated onto a master transfer function, see \autoref{fig:unpack}.


\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{Images/unpack.eps}
\caption[ unpack]{Virtual unpacking: (a) all objects in packed positions, (b)-(c)images extracted from the animation of unpacking the red doll, which
first moves towards the camera, then it flies to the top-right corner of the image; and (e) most objects are in unpacked positions}
\label{fig:unpack}
\end{figure}





Despite all these previous work, efficiently selecting the target and removing all in-between occluders is still challenging. More specifically, most existing occlusion management techniques do not simultaneously meet all following requirements: Rapidly create an unobstructed view of the target (R1), allow a flexible local exploration of the target zone (R2), keep the context in which the target is visually embedded (R3), and handle datasets where the target and occluders cannot 
be separated by transfer function manipulations (R4).

\subsubsection{Requirements}

To start with, let us recall requirements R1$,\ldots,$R4.
\begin{itemize}
\item{\textbf{R1:}} The technique should rapidly create an unobstructed view of the target, \emph{i.e.}, such a view should be created at interactive frame rates (10 fps or more) with no special pre-processing of the input volume (\emph{e.g.}, segmentation), and with minimal user input (\emph{e.g.}, using simple mouse and/or keyboard-modifier events). All above are needed to ensure that one can freely \emph{explore} the volume by activating the lens anywhere with minimal effort and seeing its effect in real-time.

\item{\textbf{R2:}} Allowing a flexible exploration of the target zone means that one can manipulate the zone in the lens in various ways to see how the target is actually embedded in its surrounding context.

\item{\textbf{R3:}} Keeping the context means that the visualization around the lens does not change significantly from what would be shown there if the lens were not activated. This is needed to maintain the user's mental map before \emph{vs} after activating the lens.

\item{\textbf{R4:}} The lens should enable the exploration of datasets where targets cannot be easily separated (isolated) from their surrounding context simply by manipulating parameters of the transfer functions (TF). One such issue is when targets and surrounding zones have similar densities; in this case, using a single global opacity TF could either render everything opaque (thus, it will be hard to visually isolate the target) or highly transparent (thus, the zone around the target will be transparent but so will be the target too).
\end{itemize}


\subsubsection{ Comparison between the requirements and the previous work }

Many occlusion management approaches have been proposed\,\cite{4483791}. Multiple viewports show the data from different perspectives\,\cite{WangBaldonado:2000:GUM:345513.345271}. This does not help when the target is strongly occluded from \emph{all} possible viewpoints (R4). Virtual X-ray methods make targets visible by turning occluders (half-)transparent\,\cite{Burns:2008:ACC:1457515.1409107}.\cite{4015450}, which interactively focuses on specific areas in the data while preserving context without visual clutter by modulating transparency. Correa and Ma\,\cite{5416704} proposed visibility-driven transfer functions (TFs) to maximize the visibility of selected data-intervals. Designing good TFs is still challenging (R2) in general: For instance, in baggage inspection, a dissimulation strategy is to hide a threat among same-density objects, so TF editing cannot easily remove occluders but keep the target (R4)\,\cite{7819413}. De-occluding a tumor from surrounding similar-density tissue in medical scans is a similar situation\, \cite{CGF:CGF12927}. 
\cite{CGF:CGF979} also considered the voxels' occurrence on the cast ray, besides their densities and positions.\cite{6787171} removed occluders in 2D images, volumes, and trail-sets by deforming (pushing them away) in a focus area. Occludes are selected based on data value-ranges, thus we have here the same limitation as ClearView (R4). \cite{Li:2012:LVV:2425296.2425325} proposed luggage virtual unpacking where targets are cleared by interactively moving away occluders. This, however, alters the \emph{context} (relative position, connectivity) where occluders occur (R3). Recently, an interactive volumetric data exploration via direct voxel manipulation was proposed, \cite{7819413}. Extending such approaches in a DVR environment to more complex deformations or changes of the data-in-focus is computationally challenging (R1).

Lenses for DVR face spatial selection and occlusion challenges. \cite{1532818} addresses these by rendering occlusions with lower opacity and magnifying pre-computed features interactively or automatically in a pre-segmented dataset. This, however, does not provide an (interactive) way to deal with similar-density occluders and targets (R4) and does not propose a local exploration of the target context (R2). GlyphLens\,\cite{7539643} removes occluding glyphs by pulling them aside through animation. However, this covers only glyph-based and not DVR visualizations. 

Lenses can create discontinuities between their inside and outside areas. Deformations can be a solution to this.
 \cite{Hsu:2011:RFM:2070781.2024165} create flexible deformations by non-linearly sampled rays to smoothly project objects at multiple levels of detail. However, this is computationally expensive and far from real-time (R1). Exploded views are used to partition a volume into several segments\, \cite{Sonnet:2004:IEA:989863.989871},\cite{4015467}. This strongly reduces occlusion but distorts the context (R3) and requires some sort of data pre-segmentation which takes time to compute (R1). \cite{Correa:2007:IDD:1313046.1313163,Correa:2006:FAV:1187627.1187827} allow one to manipulate the geometry of a data object.\cite{1250400} performed deformations using peeling to see hidden parts of the data. Such techniques remove potentially important contextual information surrounding the target, which makes it hard to see how the target is embedded in its context (R2).

Deformations can reveal specific data features by using a precomputed segmentation. \cite{7332955} proposed a deforming lens which moves streamlines to observe the inner part of streamline bundles. Other techniques performed deformations using surgical metaphors, such us \cite{4069230,Correa:2006:FAV:1187627.1187827} to show hidden parts of a volume. Such techniques do not offer tools for local manipulation of the viewpoint that allows seeing a target under multiple perspectives (R2) while keeping the global context. 

Using non-straight-line rays is another way to avoid occluders. \cite{5613463} propose curved (B{\'e}zier) rays to support this.  \cite{7120994} refine this further to create multi-perspective views. These approaches require a quite careful ray-path planning in advance, so they are not directly aimed at a lens effect (R2). Also, the target should be accessible from the viewpoint via a (possibly curved) path (R4). Also, from the presented examples, although two examples of DVR models are given, it seems these techniques mainly address de-occlusion in large 3D polygonal scenes such as terrain and city models.

\autoref{tab:methods} summarizes the main advantages and limitations of a set of volumetric exploration techniques selected from the ones mentioned above which are close to our proposal. As visible, none of the techniques scores high on all requirements. The table also lists the number of different datasets and/or use-cases these techniques were tested on. We will validate our proposal on a similar number of use-cases ( \autoref{sec:scenarios}).
\begin{table}
\centering
\large
\begin{tabular}{ |c|c|c|c|c|c| }
\hline
\textbf{Technique} & \textbf{R1} & \textbf{R2} & \textbf{R3} & \textbf{R4} & \textbf{Use-cases}\\
\hline
\hline
\cite{1250400} & + & -- & + & + & 1\\
\cite{Sonnet:2004:IEA:989863.989871} & +/- & -- & + & +/- & 5\\
\cite{1532818} & + & -- & + & +/- & 6\\
\cite{4015467} & +/- & -- & -- & + & 6\\
\cite{Correa:2006:FAV:1187627.1187827} & + & -- & + & + & 8\\
\cite{Correa:2007:IDD:1313046.1313163} & + & -- & + & + & 8\\
\cite{5613463} & + & -- & + & -- & 5\\
\cite{Hsu:2011:RFM:2070781.2024165} & -- & +/- & + & + & 4\\
\cite{6787171} & + & +/- & + & -- & 6\\
\cite{7120994} & + & -- & + & -- & 5\\
\hline
\end{tabular}
\caption{Related work selection \emph{vs} requirements R1...R4 ('+': good, '+/-': average, '--': limited) and the number of use-cases (datasets) used to demonstrate these methods.}
\label{tab:methods}
\end{table}

\subsection{summary}
In this section, we studied existing occlusion management strategies in volume rendering: the transfer function, the segmentation, deformations as well as Focus+context techniques. 
The transfer function (TF) maps volumetric data to optical properties. It is used to "view" a certain part of the volume. They are relatively easy to manipulate. However, it usually requires a good knowledge of the data set to design a good transfer function even if this can be automated through segmentation or machine learning for example.


Volume segmentation also has significant performance benefits for other ray tracing algorithms. Volume segmentation can subsequently be used to highlight structures of interest. However, it is really sensitive to the type domain of application. In fact, the parameters of the segmentation depend on the type of the data-sets (medical, industrial).  


Focus + Context methods, as well as deformations, modify the volumetric data-sets in order to reduce occlusion in different parts of the data-sets. Focus+Context (F+C) allows to manage occlusion locally (usually in a scope of a lens or a tool) while the other parts of the dataset remain unchanged. These techniques are useful when a global understanding of the volume is needed. However, most existing occlusion management techniques do not simultaneously meet all following requirements: Rapidly create an unobstructed view of the target (R1), allow a flexible local exploration of the target zone (R2), keep the context in which the target is visually embedded (R3), and handle datasets where the target and occluders cannot 
be separated by transfer function manipulations (R4).

\section{Volume rendering on mobile devices( Mixed Reality, Virtual Reality, and Augmented Reality) }

The computational power of mobile devices is much stronger, and some traditional tasks on a computer can be
handled by mobile devices. One of the most obvious areas is
graphics on mobile devices. Usually, people use vertex data to
represent polygons and combine multiple polygons to describe
the surface of an object. The surface of an object can be
reconstructed easily with polygons, but the inner structures of the object are invisible. Most of the time, such as in
3D computer games, people do not concern the structures
inside the object. However, some fields including science,
engineering, and medicine, visualizing inner structures of a volume of data is important. Therefore, direct volume rendering
(DVR) was developed to visualize volume data. As
mobile devices are getting ubiquitous, people need portable
equipment solutions for DVR. Although the computation time
of DVR is more expensive than that of polygon data, there is
still a need to implement DVR on the latest mobile devices with
new graphics technologies.

\cite{7852799} proposed a framework based on OpenGL ES and GLSL shaders. This
mobile framework has three components, namely, rendering,
histogram mapping, and transfer function design. They
implemented a one-pass shader in order to allow the system to work in real-time. High dimension transfer functions can
be easily used in our framework without losing the user
interaction intuition. The user interface and transfer function
design steps are user-friendly to the non-professional user. They proposed different interactions such as:
\begin{itemize}
\item \textbf{Draw touch mode check box}: the default drag action
on screen is to rotate the volume object. When this
check box is checked, the system will switch to draw
mode. The touch or drag action will apply color to the
corresponding position on the transfer function texture.
\item \textbf{2D transfer function check box}: If this check box is
checked, the system will enable 2D transfer functions.
\item \textbf{Compositing mode radio group}: There are four
compositing modes: X-ray, MIP, iso-surface, and full
rendering.
\item \textbf{Showing texture radio group}: There are four textures
which can be shown: back face, final, intensity, and
gradient textures.
\item \textbf{Showing texture radio group}: There are four colors that can
be selected in this control panel. One may design a
better color selector for specific the brush color.
\item \textbf{Other controls}: More controls that have not yet been
implemented in this control panel. For example, the ray
casting steps can be controlled, more steps bring higher
render quality, fewer steps bring better performance. The
pen size and sharp can also be controlled, even the 2D
transfer function image can be painted directly. Due to the
complex of the system, we have not yet implemented all
the controls.
\end{itemize}
The rendering performance of this framework in the Xray,
MIP and iso-surface modes are around 11 FPS, almost
the same. For the full rendering mode, due to its heavy
computation, the rendering performance is much slower as 2
FPS. Basically, we can achieve real-time DVR on this mobile
device by reducing unnecessary frames.



\cite{7892381} proposed Neuro Imaging in Virtual Reality (NIVR), which is a visualization suite that employs various immersive visualizations to represent
neuroimaging information in VR. Some established techniques,
such as raymarching volume visualization, are paired with newer
techniques, such as near-field rendering, to provide a broad basis
of how we can leverage VR to improve visualization and
navigation of neuroimaging data. Several of the neuroscientific
visualization approaches presented are, to our knowledge, the first
of their kind.
NIVR offers not only an exploration of neuroscientific data
visualization \ref{fig:nivr}, but also a tool to expose and educate the public
regarding recent advancements in the field of neuroimaging. By
providing an engaging experience to explore new techniques and
discoveries in neuroimaging, we hope to spark scientific interest
through a broad audience. 
\begin{figure}[th]
\centering
\includegraphics[width=\textwidth]{Figures/NIVR}
\decoRule
\caption[ NIVR]{ Near Field renderings in NIVR}
\label{fig:nivr}
\end{figure}



\cite{7938241} presented a framework for rendering 3D volumetric data
on augmented reality displays. The system exploits depth
sensors that are becoming available in most augmented reality
HMDs and mobile devices. They provided adaptations to their
framework to accommodate devices with relatively low-end
computational capabilities, as is common in mobile and HMD
platforms designed to be highly portable. The framework
achieves real-time performance across a number of platforms
and provides improvements that preserve the sense of interior
volumes being embedded inside real-world objects. 

\subsection{summary}
In this section, we studied the previous volume rendering works on mobile devices with 3 different contexts: Virtual reality, Augmented really, and Mixed reality. The potential of this technologies is really interesting even though these mobile devices usually suffer from relatively low computational power.   

\section{Conlusion}
In this Chapter, we carry out a presentation of existing work in volume rendering algorithms, occlusion management strategies, and volume rendering in mobile devices( Mixed Reality, Virtual Reality, and Augmented Reality).

 The existing algorithms to render volumetric data-sets can be categorized into two major types: Direct Volume Rendering (DVR), and Surface-fitting algorithms. Rendering a volume directly with DVR methods is usually better for visualizing softer things like flows fields and density variations, or to take advantage of the transparency. Algorithms like raycasting allow to provide results of very high quality, usually considered to provide the best image quality. 
Isosurfaces from surface-fitting methods tend to be a popular form of visualization for volume datasets since they can be rendered by a simple polygonal model, which can be drawn on the screen very quickly. They are very useful when we want to examine structures and boundaries in volume data, such as bone shape or distinct parts of the brain. It is then better to extract a triangle or tetrahedral mesh from the voxel data or implicit field. These meshes can be studied for their topology and curvature, and rendered as solid looking objects.


Many strategies can help to deal with occlusion in volumetric data-sets. The transfer function (TF) maps volumetric data to optical properties. It is used to "view" a certain part of the volume. They are relatively easy to manipulate. However, it usually requires a good knowledge of the data set to design a good transfer function even if this can be automated through segmentation or machine learning for example. Volume segmentation also has significant performance benefits for other ray tracing algorithms. Volume segmentation can subsequently be used to highlight structures of interest. However, it is really sensitive to the type domain of application. In fact, the parameters of the segmentation depend on the type of the data-sets (medical, industrial).  Focus + Context methods, as well as deformations, modify the volumetric data-sets in order to reduce occlusion in different parts of the data-sets. Focus+Context (F+C) allows to manage occlusion locally (usually in a scope of a lens or a tool) while the other parts of the dataset remain unchanged. These techniques are useful when a global understanding of the volume is needed. However, most existing occlusion management techniques do not simultaneously meet all following requirements: Rapidly create an unobstructed view of the target (R1), allow a flexible local exploration of the target zone (R2), keep the context in which the target is visually embedded (R3), and handle datasets where the target and occluders cannot 
be separated by transfer function manipulations (R4).


 The potential of AR, VR, and MR technologies is really interesting even though these mobile devices usually suffer from relatively low computational power.   